package cmd

import (
	"bufio"
	"encoding/csv"
	"fmt"
	"io"
	"os"
	"sort"
	"strconv"
	"strings"

	"gopkg.in/volatiletech/null.v6"

	"github.com/Bnei-Baruch/archive-backend/consts"

	log "github.com/Sirupsen/logrus"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
	"gopkg.in/olivere/elastic.v6"

	"github.com/Bnei-Baruch/archive-backend/search"
	"github.com/Bnei-Baruch/archive-backend/utils"
)

var logCmd = &cobra.Command{
	Use:   "log",
	Short: "log commands",
	Run:   logFn,
}

var queriesCmd = &cobra.Command{
	Use:   "queries",
	Short: "Get logged queries from ElasticSearch",
	Run:   queriesFn,
}

var clicksCmd = &cobra.Command{
	Use:   "clicks",
	Short: "Get logged clicks from ElasticSearch",
	Run:   clicksFn,
}

var latencyCmd = &cobra.Command{
	Use:   "latency",
	Short: "Get queries latency performance from ElasticSearch",
	Run:   latencyFn,
}

var latencyAggregateCmd = &cobra.Command{
	Use:   "latency_aggregate",
	Short: "Get latency statistics from latency CSV report.",
	Run:   latencyAggregateFn,
}

var elasticUrl string
var csvFile string

func init() {
	RootCmd.AddCommand(logCmd)

	logCmd.PersistentFlags().StringVar(&elasticUrl, "elastic", "", "URL of Elastic.")
	logCmd.MarkFlagRequired("elastic")
	viper.BindPFlag("elasticsearch.url", logCmd.PersistentFlags().Lookup("elastic"))

	latencyCmd.PersistentFlags().StringVar(&csvFile, "output_file", "", "CSV path to write.")
	latencyAggregateCmd.PersistentFlags().StringVar(&csvFile, "csv_file", "", "CSV file that been generated by 'log latency' command.")

	logCmd.AddCommand(queriesCmd)
	logCmd.AddCommand(clicksCmd)
	logCmd.AddCommand(latencyCmd)
	logCmd.AddCommand(latencyAggregateCmd)
}

func logFn(cmd *cobra.Command, args []string) {
	fmt.Println("Use one of the subcommands.")
}

func initLogger() *search.SearchLogger {
	log.Infof("Setting up connection to ElasticSearch: %s", elasticUrl)
	esManager := search.MakeESManager(elasticUrl)

	return search.MakeSearchLogger(esManager)
}

func appendCsvToFile(path string, records [][]string) {
	file, err := os.OpenFile(path, os.O_CREATE|os.O_APPEND, 0660)
	if err != nil {
		log.Fatalln("cannot open file: ", err)
	}
	defer file.Close()

	writeCsv(file, records)
}

func printCsv(records [][]string) {
	writeCsv(os.Stdout, records)
}

func writeCsv(dist io.Writer, records [][]string) {
	w := csv.NewWriter(dist)
	defer w.Flush()
	for _, record := range records {
		if err := w.Write(record); err != nil {
			log.Fatalln("error writing csv: ", err)
			return
		}
	}
}

func queriesFn(cmd *cobra.Command, args []string) {
	logger := initLogger()
	printCsv([][]string{[]string{
		"#", "SearchId", "Created", "Term", "Exact", "Filters",
		"Languages", "From", "Size", "SortBy", "Error", "Suggestion",
		"Deb"}})
	totalQueries := 0
	SLICES := 100
	for i := 0; i < SLICES; i++ {
		s := elastic.NewSliceQuery().Id(i).Max(SLICES)
		queries, err := logger.GetAllQueries(s)
		utils.Must(err)
		totalQueries += len(queries)
		sortedQueries := make(search.CreatedSearchLogs, 0, len(queries))
		for _, q := range queries {
			sortedQueries = append(sortedQueries, q)
		}
		sort.Sort(sortedQueries)
		records := [][]string{}
		for i, sl := range sortedQueries {
			filters, err := utils.PrintMap(sl.Query.Filters)
			utils.Must(err)
			records = append(records, []string{
				fmt.Sprintf("%d", i+1),
				sl.SearchId,
				sl.Created.Format("2006-01-02 15:04:05"),
				sl.Query.Term,
				strings.Join(sl.Query.ExactTerms, ","),
				filters,
				strings.Join(sl.Query.LanguageOrder, ","),
				fmt.Sprintf("%d", sl.From),
				fmt.Sprintf("%d", sl.Size),
				sl.SortBy,
				fmt.Sprintf("%t", sl.Error != nil),
				sl.Suggestion,
				fmt.Sprintf("%t", sl.Query.Deb),
			})
		}
		printCsv(records)
	}
	log.Infof("Found %d queries.", totalQueries)
}

func clicksFn(cmd *cobra.Command, args []string) {
	logger := initLogger()
	printCsv([][]string{[]string{
		"#", "SearchId", "Created", "Rank", "MdbUid", "Index", "ResultType"}})
	clicks, err := logger.GetAllClicks()
	utils.Must(err)
	log.Infof("Found %d clicks.", len(clicks))
	log.Info("#\tSearchId\tCreated\tRank\tMdbUid\tIndex\tType")
	sortedClicks := make(search.CreatedSearchClicks, 0, len(clicks))
	for _, q := range clicks {
		sortedClicks = append(sortedClicks, q)
	}
	sort.Sort(sortedClicks)
	records := [][]string{}
	for i, sq := range sortedClicks {

		records = append(records, []string{
			fmt.Sprintf("%d", i+1),
			sq.SearchId,
			sq.Created.Format("2006-01-02 15:04:05"),
			fmt.Sprintf("%d", sq.Rank),
			sq.MdbUid,
			sq.Index,
			sq.ResultType,
		})
	}
	printCsv(records)
}

func latencyFn(cmd *cobra.Command, args []string) {
	logger := initLogger()
	headers := []string{
		"#", "SearchId", "Term",
	}
	headers = append(headers, consts.LATENCY_LOG_OPERATIONS_FOR_SEARCH...)
	if csvFile != "" {
		appendCsvToFile(csvFile, [][]string{headers})
	} else {
		printCsv([][]string{headers})
	}
	totalQueries := 0
	SLICES := 100
	for i := 0; i < SLICES; i++ {
		s := elastic.NewSliceQuery().Id(i).Max(SLICES)
		queries, err := logger.GetLattestQueries(s, null.StringFrom("now-7d/d"), null.BoolFrom(false))
		utils.Must(err)
		totalQueries += len(queries)
		sortedQueries := make(search.CreatedSearchLogs, 0, len(queries))
		for _, q := range queries {
			sortedQueries = append(sortedQueries, q)
		}
		sort.Sort(sortedQueries)
		records := [][]string{}
		for i, sl := range sortedQueries {
			utils.Must(err)
			var latencies []string
			for _, op := range consts.LATENCY_LOG_OPERATIONS_FOR_SEARCH {
				hasOp := false
				for _, tl := range sl.ExecutionTimeLog {
					if tl.Operation == op {
						latancy := strconv.FormatInt(tl.Time, 10)
						latencies = append(latencies, latancy)
						hasOp = true
						break
					}
				}
				if !hasOp {
					latencies = append(latencies, "0")
				}
			}
			record := []string{
				fmt.Sprintf("%d", i+1),
				sl.SearchId,
				sl.Query.Term,
			}
			record = append(record, latencies...)
			records = append(records, record)
		}
		if csvFile != "" {
			appendCsvToFile(csvFile, records)
		} else {
			printCsv(records)
		}
	}
	log.Infof("Found %d queries.", totalQueries)
}

func latencyAggregateFn(cmd *cobra.Command, args []string) {

	const metaColumnsCnt = 3 //  "#","SearchId","Term"

	reader, err := os.Open(csvFile)
	r := csv.NewReader(bufio.NewReader(reader))
	records, err := r.ReadAll()
	utils.Must(err)

	opLatenciesMap := make(map[string][]int, len(consts.LATENCY_LOG_OPERATIONS_FOR_SEARCH))
	for _, op := range consts.LATENCY_LOG_OPERATIONS_FOR_SEARCH {
		opLatenciesMap[op] = make([]int, len(records)-1)
	}

	for i := 1; i < len(records); i++ { //  skip first line (headers)

		record := records[i]

		for j := metaColumnsCnt; j < len(record); j++ {
			lat, err := strconv.Atoi(strings.TrimSpace(record[j]))
			utils.Must(err)
			for opIndex, op := range consts.LATENCY_LOG_OPERATIONS_FOR_SEARCH {
				if opIndex == j-metaColumnsCnt {
					(opLatenciesMap[op])[i-1] = lat
					continue
				}
			}
		}
	}

	for opName, latencies := range opLatenciesMap {
		sort.Slice(latencies, func(i, j int) bool {
			return latencies[i] < latencies[j]
		})
		sum, max := utils.SumAndMax(latencies)
		sum95Precent := float32(sum) * 0.95
		var percentile95 int
		var calcSum float32
		for i := 0; i < len(latencies); i++ {
			calcSum += float32(latencies[i])
			if calcSum >= sum95Precent {
				percentile95 = latencies[i] //  TBD check if correct
				break
			}
		}
		avg := sum / len(latencies)
		log.Infof("%s Stage\n\nAverage: %d\nWorst: %d\n95 percentile: %d.\n\n",
			opName, avg, max, percentile95)
	}

	//  TBD 5 worst queries
}
